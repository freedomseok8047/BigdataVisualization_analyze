{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영문 분석 -> 워드 클라우드로 그리기(시각화)\n",
    "\n",
    "## 샘플 데이터 영문 학술 문서의 제목만 추출, 그 단어의 빈도 분석 시각화 \n",
    "## 데이터 수집 : Big data  키워드로 검색 후 , 해당 학술 연구 정보 서비스에서 수집 해보기\n",
    "## 조합, pandas.concat(), 정제 re 정규식, 기본적인 유효성 체크\n",
    "## 변환 : word_tokenize(), lower()\n",
    "## matplotlib.pyplot 이용하기 \n",
    "## 단어 빈도 구해주는 Counter() 이용\n",
    "\n",
    "## 비정형 빅데이터 분석을 말하고 -> 자연어 처리 (nature language processing)\n",
    "## 자연어 처리 예) 음성, 텍스트 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 관련 단어 용어 정리\n",
    "## 텍스트 분석: 자연어 처리와 데이터 마이닝 결합하여 발전되었고\n",
    "## 비정형 텍스트 데이터에서 정보를 추출하는 분석 방법\n",
    "## 분석 방법 1) 텍스트 분류 2) 텍스트 군집화 3) 감성 분석\n",
    "\n",
    "## 전처리 : 분석 작업의 정확도를 높이기 위해서 사용할 데이터를 정리하고 변환하는 작업 \n",
    "### 수행하는 작업\n",
    "### 정제 (cleaning) : 불필요한 기호, 문자 필터하는 작업, 정규식을 이용해서 작업을 함 \n",
    "### 정규화 ( normallization) : 형대가 다른 단어를 특정의 형태로 변환 작업, 대문자, 소문자 통합하는 작업, 의미가 비슷한 단어끼리 통합작업\n",
    "### 토큰화 ( tokennization) : 토큰으로 정하는 기본 단위로 분리 작업. 문장 기준 , 단어 기준이 될 수도 있다.\n",
    "### 불용어제거 ( stopword ) : 의미 있는 단어를 추출하기 위해서, 조사, 관사, 접미사, 접두사 등 제거하는 작업\n",
    "### 어간 추출 ( semming) : 단수, 복수, 진행형(시제), 분리하는 작업\n",
    "### 표제어 추출 (lemmatization) : 단어의 기본형 형태로 일반화 하는 작업\n",
    "### 예) gone -> go , am -> be , going -> go \n",
    "\n",
    "### 워드클라우드 : 텍스트 분석에서 빈도를 시각화 할 때 많이 사용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집\n",
    "# 한국교육학술정보원(KERIS)의 RISS 사이트\n",
    "# https://www.riss.kr/index.do\n",
    "# Big data 검색해보기.\n",
    "# 한 페이지당 100개씩 내보내기 엑셀 파일 간략 정보, 반복 10번\n",
    "# 1000개의 데이터에서 제목만 추출 및 분류 작업하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from nltk) (4.66.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.12 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/BigDataTest/My_Python/2-자연어처리-영문-한글-통계/test.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/BigDataTest/My_Python/2-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EC%98%81%EB%AC%B8-%ED%95%9C%EA%B8%80-%ED%86%B5%EA%B3%84/test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# !pip3 install wordcloud\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/BigDataTest/My_Python/2-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EC%98%81%EB%AC%B8-%ED%95%9C%EA%B8%80-%ED%86%B5%EA%B3%84/test.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip3 install nltk\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/user/BigDataTest/My_Python/2-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EC%98%81%EB%AC%B8-%ED%95%9C%EA%B8%80-%ED%86%B5%EA%B3%84/test.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m nltk\u001b[39m.\u001b[39mdownload()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# !pip3 install wordcloud\n",
    "!pip3 install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 12:29:49.790 Python[34552:4393428] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 관련 패키지들 임포트\n",
    "import pandas as pd \n",
    "\n",
    "# 경로 이름 지정해서 파일 처리할 때 사용하는 도구\n",
    "import glob \n",
    "# 정규 표현식에 사용하는 도구 \n",
    "import re \n",
    "# 2차원 리스트를 -> 1차원 리스트로 차원 축소시 사용하는 도구 \n",
    "from functools import reduce\n",
    "# 자연어 처리 패키지 중에서, 단어 토큰화 작업.\n",
    "from nltk.tokenize import word_tokenize\n",
    "# 불용어 처리 작업. \n",
    "from nltk.corpus import stopwords \n",
    "# 표제어 추출 \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# 단어의 빈도를 추출하는 도구. \n",
    "from collections import Counter \n",
    "import matplotlib.pyplot as plt\n",
    "# 단어의 빈도수를 시각화하는 도구, 빈도가 높을수록 글자 크기가 커짐. \n",
    "from wordcloud import STOPWORDS, WordCloud \n",
    "\n",
    "# 데이터 조합(병합)하기\n",
    "# 현재 폴더 하위에 , 받았던 엑셀 파일명 10개 선택하기\n",
    "all_files = glob.glob(\"./myCabinetExcelData*.xls\")\n",
    "all_files \n",
    "\n",
    "# 엑셀 파일 읽어서  -> 데이터 브레임 (표형태) 변환 -> 특정 리스트에 담아두기\n",
    "# 임시로 저장할 리스트 변수\n",
    "all_files_data = []\n",
    "\n",
    "# all_files 에 담겨진 엑셀 파일의 위치가 드렁있고, \n",
    "# 해당 위치의 엑셀 파일ㅇ르 읽어서, 데이터 프레임 표 형태로 변환하기\n",
    "# 임시 리스트에 담기 \n",
    "for file in all_files:\n",
    "  # 해당 엑셀 파일의 위치의 물리 파일 읽기\n",
    "  data_frame = pd.read_excel(file)\n",
    "  # 임시 리스트에 담기\n",
    "  all_files_data.append(data_frame)\n",
    "\n",
    "# 샘플 확인 해보기, 첫번째 요소 확인 해보기\n",
    "# all_files_data = [엑셀1,엑셀2,엑셀3,...]\n",
    "all_files_data[0]\n",
    "\n",
    "# 오류 발생 \n",
    "# pip3 install xlrd\n",
    "\n",
    "# 엑셀 파일 10개를 병합해서 출력해보기 \n",
    "# axis=0 , 세로 방향으로 , 밑으로 데이터를 붙이는 작업\n",
    "all_files_data_concat = pd.concat(all_files_data, axis=0, ignore_index =True)\n",
    "all_files_data_concat.shape\n",
    "\n",
    "# 병합된 파일을 csv파일로 변환하기 \n",
    "all_files_data_concat.to_csv(\"/Users/user/BigDataTest/My_Python/2-자연어처리-영문-한글-통계/riss_Bigdata.csv\", encoding = \"utf8\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
